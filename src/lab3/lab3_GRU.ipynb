{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b499a7d1",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe77299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read': 0,\n",
       " 'apple': 1,\n",
       " 'the': 2,\n",
       " 'that': 3,\n",
       " 'ate': 4,\n",
       " 'does': 5,\n",
       " 'machine': 6,\n",
       " 'Everybody': 7,\n",
       " 'learning': 8,\n",
       " 'dog': 9,\n",
       " 'book': 10,\n",
       " 'The': 11,\n",
       " 'nowadays': 12}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "training_data = [\n",
    "  (\"The dog ate the apple\".split(), [\"determiner\", \"noun\", \"verb\", \"determiner\", \"noun\"]),\n",
    "  (\"Everybody read that book\".split(), [\"noun\", \"verb\", \"determiner\", \"noun\"]),\n",
    "  (\"Everybody does machine learning nowadays\".split(), [\"noun\", \"verb\", \"noun\", \"noun\", \"adverb\"]),\n",
    "]\n",
    "\n",
    "tag_to_ind = {\n",
    "  \"determiner\": 0,\n",
    "  \"noun\": 1,\n",
    "  \"verb\": 2,\n",
    "  \"adverb\": 3,\n",
    "}\n",
    "\n",
    "ind_to_tag = {v: k for k, v in tag_to_ind.items()}\n",
    "vocab = {a: b for b, a in enumerate(set(word for sentence, _ in training_data for word in sentence))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf183fe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c5b7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUTagger(\n",
       "  (embedding): Embedding(13, 6)\n",
       "  (gru): GRU(6, 12, batch_first=True)\n",
       "  (classifier): Linear(in_features=12, out_features=4, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       "  (perplexity): Perplexity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchmetrics.text\n",
    "\n",
    "class GRUTagger(L.LightningModule):\n",
    "  def __init__(self, vocab, tag_to_ind):\n",
    "    super().__init__()\n",
    "    self.embedding = torch.nn.Embedding(len(vocab), embedding_dim=6) # word index -> embedding vector (6x1)\n",
    "    self.gru = torch.nn.GRU(input_size=6, hidden_size=12, batch_first=True)\n",
    "    self.classifier = torch.nn.Linear(in_features=12, out_features=len(tag_to_ind))\n",
    "    self.log_softmax = torch.nn.LogSoftmax(dim=1) \n",
    "    \n",
    "    self.perplexity = torchmetrics.text.Perplexity()\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, target = batch\n",
    "    embedds = self.embedding(x)\n",
    "    gru_out, _ = self.gru(embedds)\n",
    "    logits = self.classifier(gru_out)\n",
    "    log_probs = self.log_softmax(logits)\n",
    "    loss = self.perplexity(log_probs, target) # https://lightning.ai/docs/torchmetrics/stable/gallery/text/perplexity.html\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    x, target = batch\n",
    "    embedds = self.embedding(x)\n",
    "    lstm_out, _ = self.gru(embedds)\n",
    "    logits = self.classifier(lstm_out)\n",
    "    log_probs = self.log_softmax(logits)\n",
    "    loss = self.perplexity(log_probs, target)\n",
    "    self.log(\"test_perplexity\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), lr=0.1)\n",
    "  \n",
    "gru = GRUTagger(vocab, tag_to_ind)\n",
    "gru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713712cd",
   "metadata": {},
   "source": [
    "# DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd108a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TheDataModule at 0x1b3e1a80ce0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class TheDataModule(L.LightningDataModule):\n",
    "  def __init__(self, data: list[tuple[list[str], list[str]]], tags: dict[str, int], vocab: dict[str, int]):\n",
    "    super().__init__()\n",
    "    self.data = data\n",
    "    self.tags = tags\n",
    "    self.vocab = vocab\n",
    "  \n",
    "  def prepare_data(self):\n",
    "    pass\n",
    "  \n",
    "  def setup(self, stage: str):\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "      def __init__(self, data, tags, vocab):\n",
    "        self.data = data\n",
    "        self.tags = tags\n",
    "        self.vocab = vocab\n",
    "      \n",
    "      def __len__(self):\n",
    "        return len(self.data)\n",
    "      \n",
    "      def __getitem__(self, idx):\n",
    "        words, tags = self.data[idx]\n",
    "        x = torch.tensor([self.vocab[word] for word in words], dtype=torch.long)\n",
    "        y = torch.tensor([self.tags[tag] for tag in tags], dtype=torch.long)\n",
    "        return x, y\n",
    "    self.dataset = Dataset(self.data, self.tags, self.vocab)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return torch.utils.data.DataLoader(self.dataset)\n",
    "  def test_dataloader(self):\n",
    "    return torch.utils.data.DataLoader(self.dataset)\n",
    "  \n",
    "data_module = TheDataModule(training_data, tag_to_ind, vocab)\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd8c6c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35258ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightning.pytorch.trainer.trainer.Trainer at 0x1b3df91f6b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch import seed_everything as seed\n",
    "\n",
    "trainer = L.Trainer(\n",
    "  max_epochs=100,\n",
    "  deterministic=True,\n",
    "  enable_checkpointing=False,\n",
    "  logger=False,  # Disable CSVLogger and all logging\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ac3016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | embedding   | Embedding  | 78     | train\n",
      "1 | gru         | GRU        | 720    | train\n",
      "2 | classifier  | Linear     | 52     | train\n",
      "3 | log_softmax | LogSoftmax | 0      | train\n",
      "4 | perplexity  | Perplexity | 0      | train\n",
      "---------------------------------------------------\n",
      "850       Trainable params\n",
      "0         Non-trainable params\n",
      "850       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Plancha\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a72f438de2a4bed83860059170bb598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\Plancha\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d36a0da1fb4c6db856a55eb4404805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Plancha\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:512: You called `self.log('test_perplexity', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_perplexity      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1978654861450195     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_perplexity     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1978654861450195    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed(1)\n",
    "trainer.fit(gru, data_module)\n",
    "test_results = trainer.test(gru, data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35c90a",
   "metadata": {},
   "source": [
    "GRU got the lowest perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
