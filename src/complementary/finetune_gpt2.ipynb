{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2367e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Plancha\\Desktop\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyprojroot import here\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(here(\"cache/transformers\"))\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f1b192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of artificial intelligence is still in flux, but at least the main contenders are still in the process of being designed.\n",
      "\n",
      "What does this mean for you?\n",
      "\n",
      "It means that people with AI can be more productive and more productive at the same time. There's nothing wrong with that. What's more, it's not necessarily wrong to have an AI that can answer all of the social questions you're facing.\n",
      "\n",
      "And it might also mean that you don't have to worry about any of the social problems\n"
     ]
    }
   ],
   "source": [
    "# test one example\n",
    "model.eval()\n",
    "# Add padding token if not already present\n",
    "tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate with different parameters\n",
    "with torch.no_grad():\n",
    "  outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    "  )\n",
    "\n",
    "# Decode and display\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4084d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>directions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[In a heavy 2-quart saucepan, mix brown sugar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[Place chipped beef on bottom of baking dish.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[In a slow cooker, combine all ingredients. Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[Boil and debone chicken., Put bite size piece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[Combine first four ingredients and press in 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Heath Bar Pie</td>\n",
       "      <td>[Mix chopped Heath bars with whipped topping a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Victorian Baked French Toast</td>\n",
       "      <td>[Cook brown sugar, butter and corn syrup in sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Quick Swedish Meatballs</td>\n",
       "      <td>[Combine meat, bread crumbs, cheese, soup mix,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Irish Stew(Microwave)</td>\n",
       "      <td>[In 4-quart casserole, combine lamb, 1 1/4 cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Peach Salad</td>\n",
       "      <td>[Mix jello with hot water and sugar. Allow to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0             No-Bake Nut Cookies   \n",
       "1           Jewell Ball'S Chicken   \n",
       "2                     Creamy Corn   \n",
       "3                   Chicken Funny   \n",
       "4            Reeses Cups(Candy)     \n",
       "..                            ...   \n",
       "995                 Heath Bar Pie   \n",
       "996  Victorian Baked French Toast   \n",
       "997       Quick Swedish Meatballs   \n",
       "998       Irish Stew(Microwave)     \n",
       "999                   Peach Salad   \n",
       "\n",
       "                                            directions  \n",
       "0    [In a heavy 2-quart saucepan, mix brown sugar,...  \n",
       "1    [Place chipped beef on bottom of baking dish.,...  \n",
       "2    [In a slow cooker, combine all ingredients. Co...  \n",
       "3    [Boil and debone chicken., Put bite size piece...  \n",
       "4    [Combine first four ingredients and press in 1...  \n",
       "..                                                 ...  \n",
       "995  [Mix chopped Heath bars with whipped topping a...  \n",
       "996  [Cook brown sugar, butter and corn syrup in sm...  \n",
       "997  [Combine meat, bread crumbs, cheese, soup mix,...  \n",
       "998  [In 4-quart casserole, combine lamb, 1 1/4 cup...  \n",
       "999  [Mix jello with hot water and sugar. Allow to ...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset from https://recipenlg.cs.put.poznan.pl/\n",
    "import pandas as pd\n",
    "import duckdb as sql\n",
    "data = sql.query(f\"\"\"\n",
    "  select title, directions\n",
    "  from read_csv(\n",
    "    '{here('data/recipeNLG/recipeNLG.csv')}', \n",
    "    header=True,\n",
    "    delim=',',\n",
    "    types = {{\n",
    "      'title': 'VARCHAR',\n",
    "      'ingredients': 'VARCHAR[]',\n",
    "      \"directions\": 'VARCHAR[]'\n",
    "    }}\n",
    "  )\n",
    "  limit 1000\n",
    "\"\"\")\n",
    "data = data.df()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d029c8053f624851ad1df0e17ebc9e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting directions:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58520bc0f86e477e8321353711529640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting directions:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>directions</th>\n",
       "      <th>formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>1 - In a heavy 2-quart saucepan, mix brown sug...</td>\n",
       "      <td>Recipe: No-Bake Nut Cookies\\n\\nInstructions:\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>1 - Place chipped beef on bottom of baking dis...</td>\n",
       "      <td>Recipe: Jewell Ball'S Chicken\\n\\nInstructions:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>1 - In a slow cooker, combine all ingredients....</td>\n",
       "      <td>Recipe: Creamy Corn\\n\\nInstructions:\\n1 - In a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>1 - Boil and debone chicken.\\n2 - Put bite siz...</td>\n",
       "      <td>Recipe: Chicken Funny\\n\\nInstructions:\\n1 - Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>1 - Combine first four ingredients and press i...</td>\n",
       "      <td>Recipe: Reeses Cups(Candy)  \\n\\nInstructions:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Heath Bar Pie</td>\n",
       "      <td>1 - Mix chopped Heath bars with whipped toppin...</td>\n",
       "      <td>Recipe: Heath Bar Pie\\n\\nInstructions:\\n1 - Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Victorian Baked French Toast</td>\n",
       "      <td>1 - Cook brown sugar, butter and corn syrup in...</td>\n",
       "      <td>Recipe: Victorian Baked French Toast\\n\\nInstru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Quick Swedish Meatballs</td>\n",
       "      <td>1 - Combine meat, bread crumbs, cheese, soup m...</td>\n",
       "      <td>Recipe: Quick Swedish Meatballs\\n\\nInstruction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Irish Stew(Microwave)</td>\n",
       "      <td>1 - In 4-quart casserole, combine lamb, 1 1/4 ...</td>\n",
       "      <td>Recipe: Irish Stew(Microwave)  \\n\\nInstruction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Peach Salad</td>\n",
       "      <td>1 - Mix jello with hot water and sugar. Allow ...</td>\n",
       "      <td>Recipe: Peach Salad\\n\\nInstructions:\\n1 - Mix ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0             No-Bake Nut Cookies   \n",
       "1           Jewell Ball'S Chicken   \n",
       "2                     Creamy Corn   \n",
       "3                   Chicken Funny   \n",
       "4            Reeses Cups(Candy)     \n",
       "..                            ...   \n",
       "995                 Heath Bar Pie   \n",
       "996  Victorian Baked French Toast   \n",
       "997       Quick Swedish Meatballs   \n",
       "998       Irish Stew(Microwave)     \n",
       "999                   Peach Salad   \n",
       "\n",
       "                                            directions  \\\n",
       "0    1 - In a heavy 2-quart saucepan, mix brown sug...   \n",
       "1    1 - Place chipped beef on bottom of baking dis...   \n",
       "2    1 - In a slow cooker, combine all ingredients....   \n",
       "3    1 - Boil and debone chicken.\\n2 - Put bite siz...   \n",
       "4    1 - Combine first four ingredients and press i...   \n",
       "..                                                 ...   \n",
       "995  1 - Mix chopped Heath bars with whipped toppin...   \n",
       "996  1 - Cook brown sugar, butter and corn syrup in...   \n",
       "997  1 - Combine meat, bread crumbs, cheese, soup m...   \n",
       "998  1 - In 4-quart casserole, combine lamb, 1 1/4 ...   \n",
       "999  1 - Mix jello with hot water and sugar. Allow ...   \n",
       "\n",
       "                                             formatted  \n",
       "0    Recipe: No-Bake Nut Cookies\\n\\nInstructions:\\n...  \n",
       "1    Recipe: Jewell Ball'S Chicken\\n\\nInstructions:...  \n",
       "2    Recipe: Creamy Corn\\n\\nInstructions:\\n1 - In a...  \n",
       "3    Recipe: Chicken Funny\\n\\nInstructions:\\n1 - Bo...  \n",
       "4    Recipe: Reeses Cups(Candy)  \\n\\nInstructions:\\...  \n",
       "..                                                 ...  \n",
       "995  Recipe: Heath Bar Pie\\n\\nInstructions:\\n1 - Mi...  \n",
       "996  Recipe: Victorian Baked French Toast\\n\\nInstru...  \n",
       "997  Recipe: Quick Swedish Meatballs\\n\\nInstruction...  \n",
       "998  Recipe: Irish Stew(Microwave)  \\n\\nInstruction...  \n",
       "999  Recipe: Peach Salad\\n\\nInstructions:\\n1 - Mix ...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Formatting directions\")\n",
    "\n",
    "# in directions, transform list to string, with every line starting with \"1 -\"\n",
    "data['directions'] = data['directions'].progress_apply(\n",
    "  lambda x: '\\n'.join([f\"{i+1} - {step}\" for i, step in enumerate(x)])\n",
    ")\n",
    "data['formatted'] = data.progress_apply(\n",
    "  lambda row: f\"Recipe: {row['title']}\\n\\nInstructions:\\n{row['directions']}\\n{tokenizer.eos_token}\",\n",
    "  axis=1\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712dfaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: No-Bake Nut Cookies\n",
      "\n",
      "Instructions:\n",
      "1 - In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\n",
      "2 - Stir over medium heat until mixture bubbles all over top.\n",
      "3 - Boil and stir 5 minutes more. Take off heat.\n",
      "4 - Stir in vanilla and cereal; mix well.\n",
      "5 - Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\n",
      "6 - Let stand until firm, about 30 minutes.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(data[\"formatted\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0478a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# Custom Dataset class\n",
    "class RecipeDataset(Dataset):\n",
    "  def __init__(self, texts, tokenizer, max_length=512):\n",
    "    self.texts = texts\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "      \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    text = self.texts[idx]\n",
    "    encoding = self.tokenizer(\n",
    "      text,\n",
    "      truncation=True,\n",
    "      padding='max_length',\n",
    "      max_length=self.max_length,\n",
    "      return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten()\n",
    "    }\n",
    "\n",
    "dataset = RecipeDataset(\n",
    "  texts=data['formatted'].tolist(),\n",
    "  tokenizer=tokenizer,\n",
    "  max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92505d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Plancha\\AppData\\Local\\Temp\\ipykernel_35060\\3010357902.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n",
      "Number of trainable parameters: 124,440,576\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=str(here('models/gpt2-recipe-finetuned')),\n",
    "  overwrite_output_dir=True,\n",
    "  num_train_epochs=4,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer,\n",
    "  mlm=False,  # GPT-2 is not a masked language model\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=dataset,\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cabda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 55:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.926300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to: c:\\Users\\Plancha\\Desktop\\AML-homework\\models\\gpt2-recipe-finetuned\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_save_path = here('models/gpt2-recipe-finetuned')\n",
    "if Path(model_save_path).exists():\n",
    "  print(\"Loading existing fine-tuned model...\")\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_save_path)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
    "else:\n",
    "  # Start fine-tuning\n",
    "  print(\"Starting fine-tuning...\")\n",
    "  trainer.train()\n",
    "\n",
    "  # Save the fine-tuned model\n",
    "  trainer.save_model(model_save_path)\n",
    "  tokenizer.save_pretrained(model_save_path)\n",
    "  print(f\"Fine-tuned model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a73559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65e53d110944c62963edbccc5d215e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting directions:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aff658b5854f8f8c0da3ee816abcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting directions:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: Pepperoni Loaf\n",
      "\n",
      "Instructions:\n",
      "1 - Take thawed bread and roll out onto cookie sheet.\n",
      "2 - Set on door of oven preheated to 350u00b0 and let it rise for 15 to 20 minutes (until puffy).\n",
      "3 - Lay the slices of Provolone over entire loaf and then layer the pepperoni slices.\n",
      "4 - Roll bread in a jelly-like roll. Take egg white and baste bread.\n",
      "5 - Sprinkle sesame seed on top. Bake at 350u00b0 for 30 minutes.\n",
      "<|endoftext|>\n",
      "----\n",
      "----\n",
      "Recipe: Animal Crackers\n",
      "\n",
      "Instructions:\n",
      "1 - Grind oatmeal in blender until fine.\n",
      "2 - Add honey, salt, flour and soda.\n",
      "3 - Stir. Cut in butter. Add buttermilk and stir.\n",
      "4 - Roll very thin.\n",
      "5 - Cut with animal cookie cutters.\n",
      "6 - Bake at 400u00b0 until brown (10 to 12 minutes).\n",
      "<|endoftext|>\n",
      "----\n",
      "----\n",
      "Recipe: Sauerkraut\n",
      "\n",
      "Instructions:\n",
      "1 - Cook kraut and water about 10 minutes.\n",
      "2 - Fry bacon.\n",
      "3 - Add onion to\n",
      "4 - drippings and cook until onion is glazed.\n",
      "5 - Add flour and stir well.\n",
      "6 - Pour\n",
      "7 - water from kraut into gravy mixture and add sugar. Add kraut.\n",
      "8 - Salt and pepper to taste.\n",
      "9 - Let mixture come to a boil, if too thick add more water.\n",
      "<|endoftext|>\n",
      "----\n",
      "----\n",
      "Recipe: Good Cabbage Slaw\n",
      "\n",
      "Instructions:\n",
      "1 - Place alternate layers of cabbage and onions in a large bowl. Cover with sugar and celery seed.\n",
      "2 - Boil together vinegar, salt and mustard.\n",
      "3 - Add oil; heat again to boiling.\n",
      "4 - Pour over cabbage and onions.\n",
      "5 - Do not mix.\n",
      "6 - Refrigerate at least 4 hours.\n",
      "7 - Better after 24 hours.\n",
      "8 - Stir and serve.\n",
      "9 - Keeps well.\n",
      "<|endoftext|>\n",
      "----\n",
      "----\n",
      "Recipe: Roasted Potatoes With Arugula\n",
      "\n",
      "Instructions:\n",
      "1 - Preheat oven to 450u00b0.\n",
      "2 - In large roasting pan, toss potatoes with the oil and seasoning until coated.\n",
      "3 - Bake 40 minutes, stirring once, until potatoes are tender.\n",
      "4 - Place arugula in large serving bowl.\n",
      "5 - Add potato mixture and toss until arugula is wilted.\n",
      "6 - Serve hot or at room temperature.\n",
      "7 - Makes 4 servings.\n",
      "<|endoftext|>\n",
      "----\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "eval_data = sql.query(f\"\"\"\n",
    "  select title, directions\n",
    "  from read_csv(\n",
    "    '{here('data/recipeNLG/recipeNLG.csv')}', \n",
    "    header=True,\n",
    "    delim=',',\n",
    "    types = {{\n",
    "      'title': 'VARCHAR',\n",
    "      'ingredients': 'VARCHAR[]',\n",
    "      \"directions\": 'VARCHAR[]'\n",
    "    }}\n",
    "  )\n",
    "  limit 5\n",
    "  offset 1001\n",
    "\"\"\")\n",
    "eval_data = eval_data.df()\n",
    "eval_data['directions'] = eval_data['directions'].progress_apply(\n",
    "  lambda x: '\\n'.join([f\"{i+1} - {step}\" for i, step in\n",
    "  enumerate(x)])\n",
    ")\n",
    "eval_data['formatted'] = eval_data.progress_apply(\n",
    "  lambda row: f\"Recipe: {row['title']}\\n\\nInstructions:\\n{row['directions']}\\n{tokenizer.eos_token}\",\n",
    "  axis=1\n",
    ")\n",
    "eval_dataset = RecipeDataset(\n",
    "  texts=eval_data['formatted'].tolist(),\n",
    "  tokenizer=tokenizer,\n",
    "  max_length=512\n",
    ")\n",
    "for i in eval_data['formatted']:\n",
    "  print(i)\n",
    "  print(\"----\")\n",
    "  print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb1017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: Pepperoni Loaf\n",
      "\n",
      "Instructions:\n",
      "1 - Brown onion in oil and garlic.\n",
      "2 - Add flour, salt, pepper and pepper flakes.\n",
      "3 - Mix well.\n",
      "4 - Sprinkle with all ingredients and bake at 350u00b0 for 30 minutes or until a toothpick inserted in center comes out clean.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the fine-tuned model\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "inputs = tokenizer(eval_data['formatted'][0].split('\\n')[0], return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "# Decode and display\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2d9e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m   outputs = \u001b[43moriginal_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Decode and display\u001b[39;00m\n\u001b[32m     19\u001b[39m generated_text_original = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Plancha\\Desktop\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Plancha\\Desktop\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Plancha\\Desktop\\AML-homework\\.pixi\\envs\\gpu\\Lib\\site-packages\\transformers\\generation\\utils.py:3548\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3546\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3548\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3549\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   3550\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   3552\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# compare original model and fine-tuned model\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "original_model.eval()\n",
    "original_model.to(device)\n",
    "# Generate text using the original model\n",
    "inputs = tokenizer(eval_data['formatted'][0].split('\\n')[0], return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "  outputs = original_model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "  )\n",
    "# Decode and display\n",
    "generated_text_original = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text_original)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
